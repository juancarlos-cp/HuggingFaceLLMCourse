{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b4e0d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df433ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The fast path is not available because on of `(selective_state_update, causal_conv1d_fn, causal_conv1d_update)` is None. Falling back to the naive implementation. To install follow https://github.com/state-spaces/mamba/#installation and https://github.com/Dao-AILab/causal-conv1d\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FalconH1ForCausalLM(\n",
       "  (model): FalconH1Model(\n",
       "    (embed_tokens): Embedding(65537, 1280, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-65): 66 x FalconH1DecoderLayer(\n",
       "        (feed_forward): FalconH1MLP(\n",
       "          (gate_proj): Linear(in_features=1280, out_features=3072, bias=False)\n",
       "          (up_proj): Linear(in_features=1280, out_features=3072, bias=False)\n",
       "          (down_proj): Linear(in_features=3072, out_features=1280, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (mamba): FalconH1Mixer(\n",
       "          (act): SiLU()\n",
       "          (conv1d): Conv1d(2048, 2048, kernel_size=(4,), stride=(1,), padding=(3,), groups=2048)\n",
       "          (in_proj): Linear(in_features=1280, out_features=3608, bias=False)\n",
       "          (norm): FalconH1RMSNormGated()\n",
       "          (out_proj): Linear(in_features=1536, out_features=1280, bias=False)\n",
       "        )\n",
       "        (self_attn): FalconH1Attention(\n",
       "          (q_proj): Linear(in_features=1280, out_features=768, bias=False)\n",
       "          (k_proj): Linear(in_features=1280, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=1280, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=768, out_features=1280, bias=False)\n",
       "        )\n",
       "        (input_layernorm): FalconH1RMSNorm((1280,), eps=1e-05)\n",
       "        (pre_ff_layernorm): FalconH1RMSNorm((1280,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (final_layernorm): FalconH1RMSNorm((1280,), eps=1e-05)\n",
       "    (rotary_emb): FalconH1RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1280, out_features=65537, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# model_id = \"tiiuae/Falcon-H1-1.5B-Deep-Instruct\"\n",
    "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, \n",
    "    torch_dtype=torch.float16,\n",
    "    load_in_4bit=True,\n",
    "    )\n",
    "model.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccf3348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "# model.config.pad_token_id = model.config.eos_token_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc80d14f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user\n",
      "Who are you?\n",
      "assistant\n",
      "I am an artificial intelligence digital assistant created by OpenAI. My purpose is to provide information, answer questions, and assist with a wide range of tasks through natural language processing. I don'\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "\tmessages,\n",
    "\tadd_generation_prompt=True,\n",
    "\ttokenize=True,\n",
    "\treturn_dict=True,\n",
    "\treturn_tensors=\"pt\",\n",
    ").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs, max_new_tokens=40)\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4572a696",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|><|im_start|>user\\nWho are you?<|im_end|>\\n<|im_start|>assistant\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(inputs['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e0fbf9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{{bos_token}}\n",
      "{%- if tools %}\n",
      "    {{- '<|im_start|>system\\n' }}\n",
      "    {%- if messages[0].role == 'system' %}\n",
      "        {{- messages[0].content + '\\n\\n' }}\n",
      "    {%- endif %}\n",
      "    {{- \"You are a function calling AI model. You are provided with function signature within <tools> </tools> XML tags. You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions.\\n<tools>\\n\" }}\n",
      "    {%- for tool in tools %}[{{- tool | tojson }}]{%- endfor %}\n",
      "    {{- \"\\n</tools>\\nFor each function call, return a json object with function name and arguments within <tool_call> </tool_call> tags with the following schema:\\n<tool_call>\\n{'arguments': <args-dict>, 'name': <function-name>}\\n</tool_call>\\n\" }}\n",
      "{%- else %}\n",
      "    {%- if messages[0].role == 'system' %}\n",
      "        {{- '<|im_start|>system\\n' + messages[0].content + '<|im_end|>\\n' }}\n",
      "    {%- endif %}\n",
      "{%- endif %}{% for message in messages %}{%- if message.role != 'system' %}{{'<|im_start|>' + message['role'] + '\n",
      "' + message['content'] + '<|im_end|>' + '\n",
      "'}}{%- endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n",
      "' }}{% endif %}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.chat_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dacba54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca276b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    {\n",
    "        \"name\": \"get_weather\",\n",
    "        \"description\": \"Get the current weather for a given location.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"location\": {\"type\": \"string\", \"description\": \"The location name\"}\n",
    "            },\n",
    "            \"required\": [\"location\"]\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f75128a",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What’s the weather like in Paris?\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dbd0f74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tools=tools,\n",
    "    tokenize=False,            # important: we want the raw prompt to see/use it\n",
    "    add_generation_prompt=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bbbd94f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|><|im_start|>system\\nYou are a function calling AI model. You are provided with function signature within <tools> </tools> XML tags. You may call one or more functions to assist with the user query. Don\\'t make assumptions about what values to plug into functions.\\n<tools>\\n[{\"name\": \"get_weather\", \"description\": \"Get the current weather for a given location.\", \"parameters\": {\"type\": \"object\", \"properties\": {\"location\": {\"type\": \"string\", \"description\": \"The location name\"}}, \"required\": [\"location\"]}}]\\n</tools>\\nFor each function call, return a json object with function name and arguments within <tool_call> </tool_call> tags with the following schema:\\n<tool_call>\\n{\\'arguments\\': <args-dict>, \\'name\\': <function-name>}\\n</tool_call>\\n<|im_start|>user\\nWhat’s the weather like in Paris?<|im_end|>\\n<|im_start|>assistant\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c898d1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weather(location: str) -> str:\n",
    "    \"\"\"\n",
    "    Get the current weather for a given location.\n",
    "\n",
    "    Args:\n",
    "        location: The name of the location to get the weather for.\n",
    "\n",
    "    Returns:\n",
    "        A description of the current weather in the specified location.\n",
    "    \"\"\"\n",
    "    weather_prediction = f\"The weather in {location} is sunny with low temperatures. \\n\"\n",
    "    print(weather_prediction)\n",
    "    return weather_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b8310dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "output = model.generate(**inputs, max_new_tokens=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "42aff721",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'system\\nYou are a function calling AI model. You are provided with function signature within <tools> </tools> XML tags. You may call one or more functions to assist with the user query. Don\\'t make assumptions about what values to plug into functions.\\n<tools>\\n[{\"name\": \"get_weather\", \"description\": \"Get the current weather for a given location.\", \"parameters\": {\"type\": \"object\", \"properties\": {\"location\": {\"type\": \"string\", \"description\": \"The location name\"}}, \"required\": [\"location\"]}}]\\n</tools>\\nFor each function call, return a json object with function name and arguments within   tags with the following schema:\\n\\n{\\'arguments\\': <args-dict>, \\'name\\': <function-name>}\\n\\nuser\\nWhat’s the weather like in Paris?\\nassistant\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70f2e9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages=[\n",
    "    {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "caa587bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_the_thing(messages):\n",
    "\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            add_generation_prompt=True,\n",
    "            tokenize=True,\n",
    "            return_dict=True,\n",
    "            return_tensors=\"pt\",\n",
    "            ).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=40)\n",
    "    \n",
    "    text_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    return text_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98f03b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|im_start|>user\n",
      "What is the capital of France?<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "print(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84afa77e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user\n",
      "What is the capital of France?\n",
      "assistant\n",
      "The capital of France is Paris. Known for its rich history, iconic landmarks such as the Eiffel Tower, and vibrant culture, Paris is one of the most famous cities in the world\n"
     ]
    }
   ],
   "source": [
    "output_text = do_the_thing(messages)\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3978457",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e46174a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages=[\n",
    "    {\"role\": \"user\", \"content\": \"What is the weather in London right now?\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "670a5794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user\n",
      "What is the weather in London right now?\n",
      "assistant\n",
      "As\n"
     ]
    }
   ],
   "source": [
    "output_text = do_the_thing(messages)\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8937254c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9cfc9f9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.tokenization_utils_fast.PreTrainedTokenizerFast"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "89ad9f8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|end_of_text|>'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token\n",
    "tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "02a6f7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mood(location: str) -> str:\n",
    "    \"\"\"\n",
    "    Get the current mood of the populace for a given location.\n",
    "\n",
    "    Args:\n",
    "        location: The name of the location to get the mood of the local populace.\n",
    "\n",
    "    Returns:\n",
    "        A description of the current mood of the local populace in the specified location.\n",
    "    \"\"\"\n",
    "    mood_report = f\"The mood in {location} is extremely cranky. \\n\"\n",
    "    print(mood_report)\n",
    "    return mood_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ca665302",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weather(location: str) -> str:\n",
    "    \"\"\"\n",
    "    Get the current weather for a given location.\n",
    "\n",
    "    Args:\n",
    "        location: The name of the location to get the weather for.\n",
    "\n",
    "    Returns:\n",
    "        A description of the current weather in the specified location.\n",
    "    \"\"\"\n",
    "    weather_prediction = f\"The weather in {location} is sunny with low temperatures. \\n\"\n",
    "    print(weather_prediction)\n",
    "    return weather_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "23dbdf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools_list = [\n",
    "    {\n",
    "        \"name\": \"get_weather\",\n",
    "        \"description\": \"Get weather by location.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"location\": {\"type\": \"string\"},\n",
    "            },\n",
    "            \"required\": [\"location\"]\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0f6d0c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weather_agent(messages):\n",
    "        inputs = tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                tools=[get_weather, get_mood],\n",
    "                add_generation_prompt=True,\n",
    "                tokenize=True,\n",
    "                return_dict=True,\n",
    "                return_tensors=\"pt\",\n",
    "                ).to(model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "                outputs = model.generate(**inputs, max_new_tokens=40)\n",
    "                text_output = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "\n",
    "        # torch.cuda.empty_cache()\n",
    "\n",
    "        return text_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "64871a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|im_start|>system\n",
      "You are a function calling AI model. You are provided with function signature within <tools> </tools> XML tags. You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions.\n",
      "<tools>\n",
      "[{\"type\": \"function\", \"function\": {\"name\": \"get_weather\", \"description\": \"Get the current weather for a given location.\", \"parameters\": {\"type\": \"object\", \"properties\": {\"location\": {\"type\": \"string\", \"description\": \"The name of the location to get the weather for.\"}}, \"required\": [\"location\"]}, \"return\": {\"type\": \"string\", \"description\": \"A description of the current weather in the specified location.\"}}}][{\"type\": \"function\", \"function\": {\"name\": \"get_mood\", \"description\": \"Get the current mood of the populace for a given location.\", \"parameters\": {\"type\": \"object\", \"properties\": {\"location\": {\"type\": \"string\", \"description\": \"The name of the location to get the mood of the local populace.\"}}, \"required\": [\"location\"]}, \"return\": {\"type\": \"string\", \"description\": \"A description of the current mood of the local populace in the specified location.\"}}}]\n",
      "</tools>\n",
      "For each function call, return a json object with function name and arguments within <tool_call> </tool_call> tags with the following schema:\n",
      "<tool_call>\n",
      "{'arguments': <args-dict>, 'name': <function-name>}\n",
      "</tool_call>\n",
      "<|im_start|>user\n",
      "What is the weather in London right now?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<tool_call><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|>\n"
     ]
    }
   ],
   "source": [
    "output_text = weather_agent(messages)\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "814b28b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                tools=[get_weather],\n",
    "                add_generation_prompt=True,\n",
    "                tokenize=True,\n",
    "                return_dict=True,\n",
    "                return_tensors=\"pt\",\n",
    "                ).to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "67492bc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|user|>\\nWhat is the weather in London right now?</s> \\n<|assistant|>\\n'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(inputs['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2c81c2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs, max_new_tokens=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2e1bf51a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|user|>\\nWhat is the weather in London right now?</s> \\n<|assistant|>\\nAs of the time of writing, the weather in London is currently cloudy with a chance of rain. The temperature is currently 11°C (52°F) and there is a'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e6208e70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|user|>\\nWhat is the weather in London right now?</s> \\n<|assistant|>\\nAs of the time of writing, the weather in London is currently cloudy with a chance of rain. The temperature is currently 11°C (52°F) and there is a'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(outputs[0], skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b67112d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris.\n",
      "\n",
      "**Answer:** Paris\n",
      "\n",
      "#### \n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "#model_id = \"tiiuae/falcon-1b\"  # or 1.5B\n",
    "model_id = \"tiiuae/Falcon-H1-1.5B-Deep-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16).to(\"cuda\")\n",
    "\n",
    "prompt = \"The capital of France is\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs, max_new_tokens=10)\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "88a795fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the weather in Tampa is sunny with low temperatures. \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'the weather in Tampa is sunny with low temperatures. \\n'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_weather('Tampa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a5cfb0b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|><|im_start|>system\\nYou are a function calling AI model. You are provided with function signature within <tools> </tools> XML tags. You may call one or more functions to assist with the user query. Don\\'t make assumptions about what values to plug into functions.\\n<tools>\\n[{\"type\": \"function\", \"function\": {\"name\": \"get_weather\", \"description\": \"Get the current weather for a given location.\", \"parameters\": {\"type\": \"object\", \"properties\": {\"location\": {\"type\": \"string\", \"description\": \"The name of the location to get the weather for.\"}}, \"required\": [\"location\"]}, \"return\": {\"type\": \"string\", \"description\": \"A description of the current weather in the specified location.\"}}}][{\"type\": \"function\", \"function\": {\"name\": \"get_mood\", \"description\": \"Get the current mood of the populace for a given location.\", \"parameters\": {\"type\": \"object\", \"properties\": {\"location\": {\"type\": \"string\", \"description\": \"The name of the location to get the mood of the local populace.\"}}, \"required\": [\"location\"]}, \"return\": {\"type\": \"string\", \"description\": \"A description of the current mood of the local populace in the specified location.\"}}}]\\n</tools>\\nFor each function call, return a json object with function name and arguments within <tool_call> </tool_call> tags with the following schema:\\n<tool_call>\\n{\\'arguments\\': <args-dict>, \\'name\\': <function-name>}\\n</tool_call>\\n<|im_start|>user\\nWhat is the capital of France?<|im_end|>\\n<|im_start|>assistant\\nThe<|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|>'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages2=[\n",
    "    {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n",
    "]\n",
    "weather_agent(messages2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0134bd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tool metadata (not actual function — just schema info)\n",
    "tools = [{\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"get_weather\",\n",
    "        \"description\": \"Get the current weather in a given city\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"location\": {\"type\": \"string\", \"description\": \"City name\"},\n",
    "            },\n",
    "            \"required\": [\"location\"],\n",
    "        },\n",
    "    }\n",
    "}]\n",
    "\n",
    "# User question\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is the weather in Tokyo?\"},\n",
    "]\n",
    "\n",
    "# Apply chat template\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    tools=tools,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37af50b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# Generate\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=256,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e746575e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode result\n",
    "output_text = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:], skip_special_tokens=True)\n",
    "print(\"MODEL OUTPUT:\\n\", output_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
